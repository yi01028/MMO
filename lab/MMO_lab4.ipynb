{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDuUPX8YXmdUx7+L6uB87/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Реализация алгоритма Policy Iteration."],"metadata":{"id":"hdNjh2Xxiy2h"}},{"cell_type":"markdown","source":["Отчет по лабораторной работе должен содержать:\n","\n","титульный лист;\n","описание задания;\n","текст программы;\n","экранные формы с примерами выполнения программы."],"metadata":{"id":"GxavIfzDipXn"}},{"cell_type":"markdown","source":["На основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)."],"metadata":{"id":"BSQFjn0kivxL"}},{"cell_type":"markdown","source":["Код на языке Python для алгоритма итерации политики с использованием библиотеки Gym для среды CartPole-v1"],"metadata":{"id":"2O9P5lcIi8KD"}},{"cell_type":"markdown","source":["https://hrl.boyuai.com/chapter/1/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/#43-%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95\n","\n","https://blog.csdn.net/chenxy_bwave/article/details/129349086"],"metadata":{"id":"31OAjEzs4EJG"}},{"cell_type":"markdown","source":["https://github.com/ugapanyuk/courses_current/blob/main/code/rl/examples/policy_iteration.py"],"metadata":{"id":"Uw9JYXa-6VAm"}},{"cell_type":"markdown","source":["# 基于策略迭代求CliffWaking-v0最优解(python实现)"],"metadata":{"id":"rRSspI8tvOkj"}},{"cell_type":"markdown","source":["CliffWalking-v0游戏的环境设定类似于GridWorld，所以这里采用了类似于GridWorld的状态表示方法。环境类对象创建时，用一个二维数组表示网格环境中各cell的类型，“1”表示Terminate cell；“-1”表示Cliff cells；“0”表示其它cells。如下所示："],"metadata":{"id":"mBy22WQKwAWB"}},{"cell_type":"code","source":["from enum import Enum\n","import numpy as np\n","import random\n","\n","class State():\n","\n","    def __init__(self, row=-1, column=-1):\n","        self.row = row\n","        self.column = column\n","\n","    def __repr__(self):\n","        return \"<State: [{}, {}]>\".format(self.row, self.column)\n","\n","    def clone(self):\n","        return State(self.row, self.column)\n","\n","    def __hash__(self):\n","        return hash((self.row, self.column))\n","\n","    def __eq__(self, other):\n","        return self.row == other.row and self.column == other.column\n","\n","\n","class Action(Enum):\n","    # Opposite numeric values are assigned to opposite direction.\n","    # This is for the convenience of implementation in transit_func().\n","    UP    = 1\n","    DOWN  = -1\n","    LEFT  = 2\n","    RIGHT = -2\n","\n","class Environment():\n","\n","    def __init__(self, grid, move_prob=1.0):\n","        # grid is 2d-array. Its values are treated as an attribute.\n","        # Kinds of attribute is following.\n","        # grid[i][j] = 1: Terminate cell (game end)\n","        # grid[i][j] =-1: Cliff cells\n","        # grid[i][j] = 0: Other cells\n","\n","        self.grid = grid\n","        self.agent_state = State()\n","\n","        # Default reward is minus. Just like a poison swamp.\n","        # It means the agent has to reach the goal fast!\n","        self.default_reward = -1\n","\n","        # Agent can move to a selected direction in move_prob.\n","        # It means the agent will move different direction\n","        # in (1 - move_prob).\n","        # move_prob = 1.0 means agent always move in the selected direction.\n","        self.move_prob = move_prob\n","        self.reset()\n","\n","    @property\n","    def row_length(self):\n","        return len(self.grid)\n","\n","    @property\n","    def column_length(self):\n","        return len(self.grid[0])\n","\n","    @property\n","    def actions(self):\n","        return [Action.UP, Action.DOWN,\n","                Action.LEFT, Action.RIGHT]\n","\n","    @property\n","    def states(self):\n","        '''\n","        valid states.\n","        In CliffWalking-v0 games, cliff cells are not valid cells (unreachable).\n","        '''\n","        states = []\n","        for row in range(self.row_length):\n","            for column in range(self.column_length):\n","                # Cliff cells are not included to the valid state list.\n","                if not(row == (self.row_length - 1) and 0 < column < (self.column_length - 1)):\n","                    states.append(State(row, column))\n","        return states\n","\n","    def transit_func(self, state, action):\n","        \"\"\"\n","        Prob(s',r|s,a) stored in one dict[(s',reward)].\n","        \"\"\"\n","        transition_probs = {}\n","        if not self.can_action_at(state):\n","            # Already on the terminal cell.\n","            return transition_probs\n","\n","        opposite_direction = Action(action.value * -1)\n","\n","        for a in self.actions:\n","            prob = 0\n","            if a == action:\n","                prob = self.move_prob\n","            elif a != opposite_direction:\n","                prob = (1 - self.move_prob) / 2\n","\n","            next_state = self._move(state, a)\n","            if next_state.row == (self.row_length - 1) and 0 < next_state.column < (self.column_length - 1):\n","                reward = -100\n","                next_state = State(self.row_length - 1, 0) # Return to start grid when falls into cliff grid.\n","            else:\n","                reward = -1\n","            \n","            if (next_state,reward) not in transition_probs:\n","                transition_probs[(next_state,reward)] = prob\n","            else:\n","                transition_probs[(next_state,reward)] += prob\n","\n","        return transition_probs\n","\n","    def can_action_at(self, state):\n","        '''\n","        Assuming:\n","            grid[i][j] = 1: Terminate grid\n","            grid[i][j] =-1: Cliff grids\n","            grid[i][j] = 0: Other grids\n","        '''\n","        if self.grid[state.row][state.column] == 0:\n","            return True\n","        else:\n","            return False\n","\n","    def _move(self, state, action):\n","        \"\"\"\n","        Predict the next state upon the combination of {state, action}\n","        {state, action} --> next_state\n","        Called in transit_func()\n","        \"\"\"\n","        if not self.can_action_at(state):\n","            raise Exception(\"Can't move from here!\")\n","\n","        next_state = state.clone()\n","\n","        # Execute an action (move).\n","        if action == Action.UP:\n","            next_state.row -= 1\n","        elif action == Action.DOWN:\n","            next_state.row += 1\n","        elif action == Action.LEFT:\n","            next_state.column -= 1\n","        elif action == Action.RIGHT:\n","            next_state.column += 1\n","\n","        # Check whether a state is out of the grid.\n","        if not (0 <= next_state.row < self.row_length):\n","            next_state = state\n","        if not (0 <= next_state.column < self.column_length):\n","            next_state = state\n","\n","        # Entering into cliff grids is related to the correspong penalty and \n","        # reset to start grid, hence will be handled upper layer.\n","\n","        return next_state\n","\n","    def reset(self):\n","        # Locate the agent at lower left corner.\n","        self.agent_state = State(self.row_length - 1, 0)\n","        return self.agent_state\n","\n","\n","class Planner():\n","\n","    def __init__(self, env):\n","        self.env     = env\n","        self.log     = []\n","        self.V_grid  = []\n","        self.iters   = 0\n","\n","    def initialize(self):\n","        self.env.reset()\n","        self.log = []\n","\n","    def plan(self, gamma=0.9, threshold=0.0001):\n","        raise Exception(\"Planner have to implements plan method.\")\n","\n","    def transitions_at(self, state, action):\n","        '''\n","        Maybe moved to Environment in the future.\n","        '''\n","        transition_probs = self.env.transit_func(state, action)\n","        for (next_state,reward) in transition_probs:\n","            prob = transition_probs[(next_state,reward)]\n","            # reward, _ = self.env.reward_func(next_state)\n","            yield prob, next_state, reward\n","\n","    def dict_to_grid(self, state_reward_dict):\n","        \"\"\"\n","        Convert dict to 2-D array specific to grid-world-like game, for the convenience of \n","        print_value_grid(), etc.\n","        Using numpy array maybe better.\n","        \"\"\"\n","        grid = []\n","        for i in range(self.env.row_length):\n","            row = [0] * self.env.column_length\n","            grid.append(row)\n","        for s in state_reward_dict:\n","            grid[s.row][s.column] = state_reward_dict[s]\n","    \n","        return grid\n","    \n","    def print_value_grid(self):\n","        for i in range(len(self.V_grid)):\n","            for j in range(len(self.V_grid[0])):\n","                print('{0:6.3f}'.format(self.V_grid[i][j]), end=' ' )\n","            print('')\n","\n","\n","class PolicyIterationPlanner(Planner):\n","\n","    def __init__(self, env):\n","        super().__init__(env)\n","        self.policy = {}\n","\n","    def initialize(self):\n","        super().initialize()\n","        self.policy = {}\n","        actions = self.env.actions\n","        states = self.env.states\n","        for s in states:\n","            self.policy[s] = {}\n","            for a in actions:\n","                # Initialize policy.\n","                # At first, each action is taken uniformly. \n","                # Any other random initialization should be also OK, for example, gaussian distribution\n","                self.policy[s][a] = 1 / len(actions)                                    \n","\n","    def policy_evaluation(self, gamma, threshold):\n","        V = {}\n","        for s in self.env.states:\n","            # Initialize each state's expected reward.\n","            V[s] = 0\n","\n","        while True:\n","            delta = 0\n","            for s in V:\n","                expected_rewards = []\n","                for a in self.policy[s]:\n","                    action_prob = self.policy[s][a]\n","                    r = 0\n","                    for prob, next_state, reward in self.transitions_at(s, a):\n","                        r += action_prob * prob * \\\n","                             (reward + gamma * V[next_state])\n","                    expected_rewards.append(r)\n","                value = sum(expected_rewards)\n","                delta = max(delta, abs(value - V[s]))\n","                V[s] = value\n","            if delta < threshold:\n","                break\n","\n","        return V\n","\n","    def plan(self, gamma=0.9, threshold=0.0001):\n","        \"\"\"\n","        Implement the policy iteration algorithm\n","        gamma    : discount factor\n","        threshold: delta for policy evaluation convergency judge.\n","        \"\"\"\n","        self.initialize()\n","        states  = self.env.states\n","        actions = self.env.actions\n","\n","        def take_max_action(action_value_dict):\n","            return max(action_value_dict, key=action_value_dict.get)\n","\n","        while True:\n","            update_stable = True\n","            # Estimate expected rewards under current policy.\n","            V = self.policy_evaluation(gamma, threshold)\n","            self.log.append(self.dict_to_grid(V))\n","\n","            for s in states:\n","                # Get an action following to the current policy.\n","                policy_action = take_max_action(self.policy[s])\n","\n","                # Compare with other actions.\n","                action_rewards = {}\n","                for a in actions:\n","                    r = 0\n","                    for prob, next_state, reward in self.transitions_at(s, a):\n","                        r += prob * (reward + gamma * V[next_state])\n","                    action_rewards[a] = r\n","                best_action = take_max_action(action_rewards)\n","                if policy_action != best_action:\n","                    update_stable = False\n","\n","                # Update policy (set best_action prob=1, otherwise=0 (greedy))\n","                for a in self.policy[s]:\n","                    prob = 1 if a == best_action else 0\n","                    self.policy[s][a] = prob\n","\n","            # Turn dictionary to grid\n","            self.V_grid = self.dict_to_grid(V)\n","            self.iters = self.iters + 1\n","            print('PolicyIteration: iters = {0}'.format(self.iters))\n","            self.print_value_grid()\n","            print('******************************')\n","\n","            if update_stable:\n","                # If policy isn't updated, stop iteration\n","                break\n","\n","    def print_policy(self):\n","        print('PolicyIteration: policy = ')\n","        actions = self.env.actions\n","        states  = self.env.states\n","        for s in states:\n","            print('\\tstate = {}'.format(s))\n","            for a in actions:\n","                print('\\t\\taction = {0}, prob = {1}'.format(a,self.policy[s][a]))\n","\n","        # Optimal actions\n","        action_array = []\n","        for i in range(self.env.row_length):\n","            row = [0] * self.env.column_length\n","            action_array.append(row)\n","        for s in states:\n","            max_prob = -1                  \n","            for a in actions:\n","                if self.policy[s][a] > max_prob:\n","                    max_prob = self.policy[s][a]\n","                    opt_action = a\n","            action_array[s.row][s.column] = opt_action.value\n","        \n","        print('PolicyIteration: optimal policy = ')\n","        for i in range(self.env.row_length):\n","            print(\"========================\")\n","            for j in range(self.env.column_length):\n","                if action_array[i][j] == Action.UP.value:\n","                    print('  UP   ', end='')\n","                elif action_array[i][j] == Action.DOWN.value:\n","                    print(' DOWN  ', end='')\n","                elif action_array[i][j] == Action.LEFT.value:\n","                    print(' LEFT  ', end='')\n","                elif action_array[i][j] == Action.RIGHT.value:\n","                    print(' RIGHT ', end='')\n","                else:\n","                    print('   X   ', end='')\n","            print('')\n","                \n","if __name__ == \"__main__\":\n","\n","    # Create grid environment\n","    grid = [\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","    ]\n","    grid[3][11] = 1 # Terminate cell\n","    for k in range(1,11):\n","        grid[3][11] = -1  # Cliff cells\n","        \n","    # # A smaller grid environment, only for the convenience of debug.\n","    # grid = [\n","    #     [0,  0, 0],\n","    #     [0,  0, 0],\n","    #     [0, -1, 1]\n","    # ]    \n","    \n","    env2 = Environment(grid)\n","    policyIterPlanner = PolicyIterationPlanner(env2)\n","    policyIterPlanner.plan(0.9,0.001)\n","    policyIterPlanner.print_value_grid()    \n","    policyIterPlanner.print_policy()    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAItTMX-ycIm","executionInfo":{"status":"ok","timestamp":1686443594967,"user_tz":-180,"elapsed":2346,"user":{"displayName":"shaye loo","userId":"09210147428128921900"}},"outputId":"ea97cea4-39c5-4341-b839-e6796c264a4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PolicyIteration: iters = 1\n","-53.260 -56.451 -59.258 -60.959 -61.784 -62.002 -61.690 -60.678 -58.491 -54.352 -47.647 -40.062 \n","-69.298 -77.481 -82.260 -84.486 -85.408 -85.646 -85.366 -84.377 -81.997 -76.633 -65.260 -45.840 \n","-103.511 -131.908 -139.933 -142.422 -143.233 -143.428 -143.247 -142.525 -140.488 -134.536 -115.483 -48.126 \n","-150.891  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 2\n","-9.991 -9.992 -9.993 -9.994 -9.994 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 \n","-9.992 -9.993 -9.994 -9.994 -9.995 -9.992 -9.992 -9.992 -9.992 -9.992 -9.992 -9.992 \n","-9.993 -9.994 -9.994 -9.995 -9.995 -9.993 -9.993 -9.993 -9.993 -9.993 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 3\n","-9.991 -9.992 -9.993 -9.994 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 \n","-9.992 -9.993 -9.994 -9.994 -9.992 -9.992 -9.992 -9.992 -9.992 -9.992 -2.710 -1.900 \n","-9.993 -9.994 -9.994 -9.995 -9.993 -9.993 -9.993 -9.993 -9.993 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 4\n","-9.991 -9.992 -9.993 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -3.439 -2.710 \n","-9.992 -9.993 -9.994 -9.992 -9.992 -9.992 -9.992 -9.992 -9.992 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -9.994 -9.993 -9.993 -9.993 -9.993 -9.993 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 5\n","-9.991 -9.992 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -9.992 -9.992 -9.992 -9.992 -9.992 -9.992 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -9.993 -9.993 -9.993 -9.993 -9.993 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 6\n","-9.991 -9.992 -9.991 -9.991 -9.991 -9.991 -9.991 -9.991 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -9.992 -9.992 -9.992 -9.992 -9.992 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -9.993 -9.993 -9.993 -9.993 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 7\n","-9.991 -9.992 -9.991 -9.991 -9.991 -9.991 -9.991 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -9.992 -9.992 -9.992 -9.992 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -9.993 -9.993 -9.993 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 8\n","-9.991 -9.992 -9.991 -9.991 -9.991 -9.991 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -9.992 -9.992 -9.992 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -9.993 -9.993 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 9\n","-9.991 -9.992 -9.991 -9.991 -9.991 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -9.992 -9.992 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -9.993 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 10\n","-9.991 -9.992 -9.991 -9.991 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -9.992 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -9.994 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 11\n","-9.991 -9.992 -9.991 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -9.993 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-9.993 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-9.994  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 12\n","-9.991 -9.992 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-9.992 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 13\n","-9.991 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","PolicyIteration: iters = 14\n","-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","******************************\n","-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","PolicyIteration: policy = \n","\tstate = <State: [0, 0]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 1]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 2]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 3]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 4]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 5]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 6]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 7]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 8]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 9]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 10]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [0, 11]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 0]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 1]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 2]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 3]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 4]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 5]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 6]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 7]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 8]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 9]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 10]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [1, 11]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [2, 0]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 1]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 2]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 3]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 4]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 5]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 6]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 7]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 8]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 9]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 10]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 1\n","\tstate = <State: [2, 11]>\n","\t\taction = Action.UP, prob = 0\n","\t\taction = Action.DOWN, prob = 1\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [3, 0]>\n","\t\taction = Action.UP, prob = 1\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","\tstate = <State: [3, 11]>\n","\t\taction = Action.UP, prob = 1\n","\t\taction = Action.DOWN, prob = 0\n","\t\taction = Action.LEFT, prob = 0\n","\t\taction = Action.RIGHT, prob = 0\n","PolicyIteration: optimal policy = \n","========================\n"," DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN  \n","========================\n"," DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN   DOWN  \n","========================\n"," RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  RIGHT  DOWN  \n","========================\n","  UP      X      X      X      X      X      X      X      X      X      X     UP   \n"]}]},{"cell_type":"markdown","source":["# 书上的"],"metadata":{"id":"R0kYAkZU32f9"}},{"cell_type":"code","source":["import copy\n","\n","class CliffWalkingEnv:\n","    \"\"\" 悬崖漫步环境\"\"\"\n","    def __init__(self, ncol=12, nrow=4):\n","        self.ncol = ncol  # 定义网格世界的列\n","        self.nrow = nrow  # 定义网格世界的行\n","        # 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励\n","        self.P = self.createP()\n","\n","    def createP(self):\n","        # 初始化\n","        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]\n","        # 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n","        # 定义在左上角\n","        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n","        for i in range(self.nrow):\n","            for j in range(self.ncol):\n","                for a in range(4):\n","                    # 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0\n","                    if i == self.nrow - 1 and j > 0:\n","                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0,\n","                                                    True)]\n","                        continue\n","                    # 其他位置\n","                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))\n","                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))\n","                    next_state = next_y * self.ncol + next_x\n","                    reward = -1\n","                    done = False\n","                    # 下一个位置在悬崖或者终点\n","                    if next_y == self.nrow - 1 and next_x > 0:\n","                        done = True\n","                        if next_x != self.ncol - 1:  # 下一个位置在悬崖\n","                            reward = -100\n","                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]\n","        return P"],"metadata":{"id":"fFuktTUL2-N_","executionInfo":{"status":"ok","timestamp":1686528705231,"user_tz":-180,"elapsed":2,"user":{"displayName":"shaye loo","userId":"09210147428128921900"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class PolicyIteration:\n","    \"\"\" 策略迭代算法 \"\"\"\n","    def __init__(self, env, theta, gamma):\n","        self.env = env\n","        self.v = [0] * self.env.ncol * self.env.nrow  # 初始化价值为0\n","        self.pi = [[0.25, 0.25, 0.25, 0.25]\n","                   for i in range(self.env.ncol * self.env.nrow)]  # 初始化为均匀随机策略\n","        self.theta = theta  # 策略评估收敛阈值\n","        self.gamma = gamma  # 折扣因子\n","\n","    def policy_evaluation(self):  # 策略评估\n","        cnt = 1  # 计数器\n","        while 1:\n","            max_diff = 0\n","            new_v = [0] * self.env.ncol * self.env.nrow\n","            for s in range(self.env.ncol * self.env.nrow):\n","                qsa_list = []  # 开始计算状态s下的所有Q(s,a)价值\n","                for a in range(4):\n","                    qsa = 0\n","                    for res in self.env.P[s][a]:\n","                        p, next_state, r, done = res\n","                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n","                        # 本章环境比较特殊,奖励和下一个状态有关,所以需要和状态转移概率相乘\n","                    qsa_list.append(self.pi[s][a] * qsa)\n","                new_v[s] = sum(qsa_list)  # 状态价值函数和动作价值函数之间的关系\n","                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))\n","            self.v = new_v\n","            if max_diff < self.theta: break  # 满足收敛条件,退出评估迭代\n","            cnt += 1\n","        print(\"Strategy evaluation completed after %d round\" % cnt)\n","\n","    def policy_improvement(self):  # 策略提升\n","        for s in range(self.env.nrow * self.env.ncol):\n","            qsa_list = []\n","            for a in range(4):\n","                qsa = 0\n","                for res in self.env.P[s][a]:\n","                    p, next_state, r, done = res\n","                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n","                qsa_list.append(qsa)\n","            maxq = max(qsa_list)\n","            cntq = qsa_list.count(maxq)  # 计算有几个动作得到了最大的Q值\n","            # 让这些动作均分概率\n","            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]\n","        print(\"Strategy enhancement completed\")\n","        return self.pi\n","\n","    def policy_iteration(self):  # 策略迭代\n","        while 1:\n","            self.policy_evaluation()\n","            old_pi = copy.deepcopy(self.pi)  # 将列表进行深拷贝,方便接下来进行比较\n","            new_pi = self.policy_improvement()\n","            if old_pi == new_pi: break"],"metadata":{"id":"3N86Id9a3DGt","executionInfo":{"status":"ok","timestamp":1686528707834,"user_tz":-180,"elapsed":221,"user":{"displayName":"shaye loo","userId":"09210147428128921900"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def print_agent(agent, action_meaning, disaster=[], end=[]):\n","    print(\"Status Value：\")\n","    for i in range(agent.env.nrow):\n","        for j in range(agent.env.ncol):\n","            # 为了输出美观,保持输出6个字符\n","            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]), end=' ')\n","        print()\n","\n","    print(\"Strategies：\")\n","    for i in range(agent.env.nrow):\n","        for j in range(agent.env.ncol):\n","            # 一些特殊的状态,例如悬崖漫步中的悬崖\n","            if (i * agent.env.ncol + j) in disaster:\n","                print('****', end=' ')\n","            elif (i * agent.env.ncol + j) in end:  # 目标状态\n","                print('EEEE', end=' ')\n","            else:\n","                a = agent.pi[i * agent.env.ncol + j]\n","                pi_str = ''\n","                for k in range(len(action_meaning)):\n","                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n","                print(pi_str, end=' ')\n","        print()\n","\n","\n","env = CliffWalkingEnv()\n","action_meaning = ['^', 'v', '<', '>']\n","theta = 0.001\n","gamma = 0.9\n","agent = PolicyIteration(env, theta, gamma)\n","agent.policy_iteration()\n","print_agent(agent, action_meaning, list(range(37, 47)), [47])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9NKVZsD3Kpl","executionInfo":{"status":"ok","timestamp":1686528710335,"user_tz":-180,"elapsed":206,"user":{"displayName":"shaye loo","userId":"09210147428128921900"}},"outputId":"98c86ec8-6d8d-44f8-b5b1-3c05996dabbb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Strategy evaluation completed after 60 round\n","Strategy enhancement completed\n","Strategy evaluation completed after 72 round\n","Strategy enhancement completed\n","Strategy evaluation completed after 44 round\n","Strategy enhancement completed\n","Strategy evaluation completed after 12 round\n","Strategy enhancement completed\n","Strategy evaluation completed after 1 round\n","Strategy enhancement completed\n","Status Value：\n","-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n","-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n","-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n","-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n","Strategies：\n","ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n","ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n","ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n","^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"]}]}]}